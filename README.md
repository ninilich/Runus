# О проекте
Данный репозиторий содержит исходные коды моего демо-проекта Runus.

Идея проекта - на регулярной основе собирать новости из RSS-лент популярных сайтов, анализировать их текст, выделять из них именованные сущности (локации, организации, персоны) и собирать статистику: где, когда и сколько раз данная именованная сущность была упомянута в новостях.

# Архитектура
На выбор способа реализации повлияли следующие моменты:
- Необходимость постоянного непрерывного функционирования системы
- Развертывание исключительно в облаке
- Использование open source продуктов
- Модульность (возможность разработки небольшими частями)

Общая функциональная схема решения представлена на рисунке ниже:
![Функциональная схема](https://github.com/ninilich/Runus/raw/master/diagram-1.png)

Перечень используемых облачных сервисов:
- AWS S3 для временного хранения XML-файлов
- GCP SQL для развертывания БД Postgres
- GCP Compute Engine в качестве VPS
- На GCP Compute Engine в докер-контейнерах запущены Airflow, Python и Metabase 

Но можно развернуть и все локально без переписывания кода (за исключением использования AWS S3)

# Инструкция по развертыванию
1. Клонировать репозиторий
2. Собрать docker image
```
docker build -t runus:1.0 .
```
3. В файле ./python/cfg_settings.py прописать параметры подключения к БД Postgres , данные экканутра AWS и S3-бакета
4. При необходимости в файле ./airflow/dags/cfg_dags.py скорректировать список RSS-каналов, интервал их запуска и START_DATE
5. В БД Postgres создать все таблицы и хранимые процедуры из ./postgres/ и выполнить скрипты по первоначальному заполнению справочников ref_
6. Запустить контейнеры 
```
docker-compose up -d
```
7. В вэб-интерфейсе Airflow (host_name:8080) в "Admin - Connections" создать подключение к БД Postgres с именем postgres_runus 
8. Включить все dag'и через вэб-интерфейс Airflow
9. Через Metabase (host_name:3000) можно посмотреть дэфолтный дашбод.
